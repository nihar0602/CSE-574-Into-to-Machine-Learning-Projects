{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sVmNAYcBpgtH"
   },
   "source": [
    "# CSE474/574 - Programming Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "63BLNK8JpgtI"
   },
   "source": [
    "## Part 1 - Sentiment Analysis\n",
    "\n",
    "In the code provided below, you need to add code wherever specified by `TODO:`. \n",
    "\n",
    "> You will be using a Python collection class - `Counter` to maintain the word counts. \n",
    "\n",
    "> See https://docs.python.org/2/library/collections.html for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qdn2eG4xpgtJ"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8wGCiNUTpgtM"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'reviews.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1c6f92855a8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read data files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'reviews.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# What we know!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mreviews_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'labels.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# What we WANT to know!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'reviews.txt'"
     ]
    }
   ],
   "source": [
    "# read data files \n",
    "g = open('reviews.txt','r') # What we know!\n",
    "reviews_all = list(map(lambda x:x[:-1],g.readlines()))\n",
    "g.close()\n",
    "g = open('labels.txt','r') # What we WANT to know!\n",
    "sentiments_all = list(map(lambda x:x[:-1].upper(),g.readlines()))\n",
    "g.close()\n",
    "\n",
    "# load vocabulary\n",
    "g = open('vocab.txt','r')\n",
    "vocab = [s.strip() for s in g.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YnMrE_UBpgtO"
   },
   "source": [
    "The data is a set of 25000 movie reviews, along with a `POSITIVE` or `NEGATIVE` sentiment label assigned to the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "S7X7I438pgtP",
    "outputId": "bfd1b989-a05a-4914-ac2d-07c639aeb4d9"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentiments_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a9adc08fd8a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check out sample reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'A {} review:'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiments_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nA {} review:'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiments_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentiments_all' is not defined"
     ]
    }
   ],
   "source": [
    "# Check out sample reviews\n",
    "print('A {} review:'.format(sentiments_all[0]))\n",
    "print(reviews_all[0])\n",
    "print('\\nA {} review:'.format(sentiments_all[1]))\n",
    "print(reviews_all[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cflbOZvkpgtS"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reviews_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1715f2813541>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# split into training and test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreviews_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreviews_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviews_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m24000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreviews_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m24000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msentiments_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentiments_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiments_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m24000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentiments_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m24000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reviews_all' is not defined"
     ]
    }
   ],
   "source": [
    "# split into training and test data\n",
    "reviews_train,reviews_test = reviews_all[0:24000],reviews_all[24000:]\n",
    "sentiments_train,sentiments_test = sentiments_all[0:24000],sentiments_all[24000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qfrn1nxcpgtV"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reviews_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b75ff5fc1462>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# based on the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtotal_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reviews_train' is not defined"
     ]
    }
   ],
   "source": [
    "# maintain Counter objects to store positive, negative and total counts for\n",
    "# all the words present in the positive, negative and total reviews.\n",
    "positive_word_count = Counter()\n",
    "negative_word_count = Counter()\n",
    "total_counts = Counter()\n",
    "# TODO: Loop over all the words in the vocabulary\n",
    "# and increment the counts in the appropriate counter objects\n",
    "# based on the training data\n",
    "\n",
    "for i, review in enumerate(reviews_train):\n",
    "    for word in review.split(' '):\n",
    "        total_counts[word] += 1\n",
    "        if sentiments_train[i] == \"POSITIVE\":\n",
    "            positive_word_count[word] += 1\n",
    "        else:\n",
    "            negative_word_count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wuxTMyzUpgtX"
   },
   "outputs": [],
   "source": [
    "# maintain a Counter object to store positive to negative ratios \n",
    "pos_neg_ratios = Counter()\n",
    "\n",
    "# Calculate the ratios of positive and negative uses of the most common words\n",
    "# Consider words to be \"common\" if they've been used at least 100 times\n",
    "for term,cnt in list(total_counts.most_common()):\n",
    "    if(cnt > 100):\n",
    "        # TODO: Code for calculating the ratios (remove the next line)\n",
    "        neg = negative_word_count[term]\n",
    "        neg = 1 if neg == 0 else neg\n",
    "\n",
    "        pos_neg_ratios[term] = positive_word_count[term] / neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "xX-2yuoIpgtZ",
    "outputId": "b78bfb93-fcb3-4100-9516-7930345b064c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos-to-neg ratio for 'the' = 1.0618582280413789\n",
      "Pos-to-neg ratio for 'amazing' = 4.031496062992126\n",
      "Pos-to-neg ratio for 'terrible' = 0.17256637168141592\n"
     ]
    }
   ],
   "source": [
    "print(\"Pos-to-neg ratio for 'the' = {}\".format(pos_neg_ratios[\"the\"]))\n",
    "print(\"Pos-to-neg ratio for 'amazing' = {}\".format(pos_neg_ratios[\"amazing\"]))\n",
    "print(\"Pos-to-neg ratio for 'terrible' = {}\".format(pos_neg_ratios[\"terrible\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_3yS5VK2pgtc"
   },
   "outputs": [],
   "source": [
    "# take a log of the ratio\n",
    "for word,ratio in pos_neg_ratios.most_common():\n",
    "    pos_neg_ratios[word] = np.log(ratio)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "rpeIdeSDpgte",
    "outputId": "2903de74-5159-4354-e866-930329d3f390"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the distribution of the log-ratio scores\n",
    "scores = np.array(list(pos_neg_ratios.values()))\n",
    "vocab_selected = list(pos_neg_ratios.keys())\n",
    "\n",
    "h = plt.hist(scores,bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "98oA_Cp8pgti"
   },
   "source": [
    "The above histogram should give you an idea about the distribution of the scores.\n",
    "\n",
    "Notice how the scores are distributed around 0. A word with score 0 can be considered as `neutral`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "id": "ciQ3LQ1Kpgtj",
    "outputId": "cd4ee84c-beb4-4558-a74b-386e2972c1f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "realize\n",
      "hands\n",
      "extreme\n",
      "beat\n",
      "onto\n",
      "psycho\n",
      "test\n",
      "obsessed\n",
      "choose\n",
      "speech\n"
     ]
    }
   ],
   "source": [
    "# Print few words with neutral score\n",
    "for ind in np.where(scores == 0)[0][0:10]:\n",
    "    print(vocab_selected[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UEtlY-66pgtn"
   },
   "source": [
    "**APPROACH 1** Implement a simple non-machine learning that only uses the log-ratios to determine if a review is positive or negative. This function will be applied to the test data to calculate the accuracy of the model. \n",
    "\n",
    "_See the assignment document for hints._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2c_nxQH6pgto"
   },
   "outputs": [],
   "source": [
    "def nonml_classifier(review,pos_neg_ratios):\n",
    "    '''\n",
    "    Function that determines the sentiment for a given review.\n",
    "    \n",
    "    Inputs:\n",
    "      review - A text containing a movie review\n",
    "      pos_neg_ratios - A Counter object containing frequent words\n",
    "                       and corresponding log positive-negative ratio\n",
    "    Return:\n",
    "      sentiment - 'NEGATIVE' or 'POSITIVE'\n",
    "    '''\n",
    "    # TODO: Implement the algorithm here. Change the next line.\n",
    "    total = sum(filter(lambda x : abs(x) >= 0.65, [pos_neg_ratios[word] for word in review.split(' ')]))\n",
    "    return 'NEGATIVE' if total < 0 else 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "94Utk4vFpgts",
    "outputId": "da4aa8cb-05ff-4ec5-ae22-66805aaa9e5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model = 0.821\n"
     ]
    }
   ],
   "source": [
    "predictions_test = []\n",
    "for r in reviews_test:\n",
    "    l = nonml_classifier(r,pos_neg_ratios)\n",
    "    predictions_test.append(l)\n",
    "\n",
    "# calculate accuracy\n",
    "correct = 0\n",
    "for l,p in zip(sentiments_test,predictions_test):\n",
    "    if l == p:\n",
    "        correct = correct + 1\n",
    "\n",
    "print('Accuracy of the model = {}'.format(correct/len(sentiments_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y5egVHbtpgtw"
   },
   "source": [
    "**Approach 2** Implement a neural network for sentiment classification. \n",
    "\n",
    "> ### System Configuration\n",
    "This part requires you to use a computer with `tensorflow` library installed. More information is available here - https://www.tensorflow.org.\n",
    "`\n",
    "You are allowed to implement the project on your personal computers using `Python 3.4 or above. You will need `numpy` and `scipy` libraries. If you need to use departmental resources, you can use **metallica.cse.buffalo.edu**, which has `Python 3.4.3` and the required libraries installed. \n",
    "\n",
    "> Students attempting to use the `tensorflow` library have two options: \n",
    "1. Install `tensorflow` on personal machines. Detailed installation information is here - https://www.tensorflow.org/. Note that, since `tensorflow` is a relatively new library, you might encounter installation issues depending on your OS and other library versions. We will not be providing any detailed support regarding `tensorflow` installation. If issues persist, we recommend using option 2. \n",
    "2. Use **metallica.cse.buffalo.edu**. If you are registered into the class, you should have an account on that server. The server already has Python 3.4.3 and TensorFlow 0.12.1 installed. Please use /util/bin/python for Python 3. \n",
    "3. To maintain a ssh connection for a long-running task on a remote machine, use tools like `screen`. For more information: https://linuxize.com/post/how-to-use-linux-screen/ \n",
    "4. For running jupyter-notebook over a remote machine find information on: https://fizzylogic.nl/2017/11/06/edit-jupyter-notebooks-over-ssh/\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FshlTt_Ypgtx"
   },
   "outputs": [],
   "source": [
    "def create_input_vector(review,word2index):\n",
    "    '''\n",
    "    Function to count how many times each word is used in the given review,\n",
    "    # and then store those counts at the appropriate indices inside x.\n",
    "    '''\n",
    "    vocab_size = len(word2index)\n",
    "    x = np.zeros((1, vocab_size))\n",
    "    for w in review.split(' '):\n",
    "        if w in word2index.keys():\n",
    "            x[0][word2index[w]] += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cg4ZKauNpgt1"
   },
   "outputs": [],
   "source": [
    "def find_ignore_words(pos_neg_ratios):\n",
    "    '''\n",
    "    Function to identify words to ignore from the vocabulary\n",
    "    '''\n",
    "    ignore_words = []\n",
    "    # TODO: Complete the implementation of find_ignore_words\n",
    "    for w, val in pos_neg_ratios.most_common():\n",
    "      if abs(val) < 0.2:\n",
    "        ignore_words.append(w)\n",
    "\n",
    "    return ignore_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EZA7e-WApgt4"
   },
   "outputs": [],
   "source": [
    "# create a word2index mapping from word to an integer index\n",
    "word2index = {}\n",
    "ignore_words = find_ignore_words(pos_neg_ratios)\n",
    "vocab_selected = list(set(vocab_selected).difference(set(ignore_words)))\n",
    "for i,word in enumerate(vocab_selected):\n",
    "    if word not in ignore_words:\n",
    "        word2index[word] = i\n",
    "vocab_size = len(word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nTakNPl3pgt-"
   },
   "source": [
    "#### Generate .hdf5 files from the processed data\n",
    "Given that the data is moderately large sized, the `hdf5` file format provides a more efficient file representation for further processing. See [here](https://anaconda.org/anaconda/hdf5) for more details and installation instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kcmpQoY6pgt_"
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eFtEYxGipguD"
   },
   "outputs": [],
   "source": [
    "# Run the script once to generate the file \n",
    "# delete the exiting 'data1.hdf5' file before running it again to avoid error \n",
    "labels_train = np.zeros((len(sentiments_train), 2), dtype=int)\n",
    "labels_test = np.zeros((len(sentiments_test), 2), dtype=int)\n",
    "\n",
    "with h5py.File('data1.hdf5', 'w') as hf:\n",
    "    hf.create_dataset('data_train', (labels_train.shape[0], vocab_size), np.int16)\n",
    "    hf.create_dataset('data_test', (labels_test.shape[0], vocab_size), np.int16)\n",
    "    # create training data\n",
    "    for i,(r,l) in enumerate(zip(reviews_train, sentiments_train)):\n",
    "        hf[\"data_train\"][i] = create_input_vector(r,word2index)\n",
    "        # one-hot encoding\n",
    "        if l == 'NEGATIVE':\n",
    "            labels_train[i, 0] = 1\n",
    "        else:\n",
    "            labels_train[i, 1] = 1\n",
    "    # create test data\n",
    "    for i,(r,l) in enumerate(zip(reviews_test, sentiments_test)):\n",
    "        hf[\"data_test\"][i] = create_input_vector(r,word2index)\n",
    "        # one-hot encoding\n",
    "        if l == 'NEGATIVE':\n",
    "            labels_test[i, 0] = 1\n",
    "        else:\n",
    "            labels_test[i, 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DFY7hqp_LqML"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ugZ82leUpguF"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "tf.compat.v1.random.set_random_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pn3utqZvpguI"
   },
   "outputs": [],
   "source": [
    "# parameters of the network\n",
    "learning_rate = 0.01\n",
    "batch_size = 400\n",
    "num_epochs = 50\n",
    "n_input = vocab_size\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4kR7EBpypguJ"
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights and biases in Tensorflow according to the parameters set above\n",
    "n_hidden_1 = 10  # 1st layer number of neurons\n",
    "weights = {\n",
    "\t'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "\t'out1': tf.Variable(tf.random_normal([n_hidden_1, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "\t'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "\t'out2': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4kLZhk9OpguL"
   },
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x):\n",
    "    # define the layers of a single layer perceptron\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    out_layer = tf.nn.sigmoid(tf.matmul(layer_1, weights['out1']) + biases['out2'])\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TMlCgAkPpguM"
   },
   "outputs": [],
   "source": [
    "logits = multilayer_perceptron(X)\n",
    "# Define loss(softmax_cross_entropy_with_logits) and optimizer(AdamOptimizer)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ejA6_9ipguO"
   },
   "outputs": [],
   "source": [
    "# for some macosx installations, conflicting copies of mpilib causes trouble with tensorflow.\n",
    "# use the following two lines to resolve that issue\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 914
    },
    "colab_type": "code",
    "id": "0czE8eJopguQ",
    "outputId": "9800a5a0-e8ca-40e4-dece-b59b8a65bdb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.592583, Test_acc: 0.682500\n",
      "Train acc: 0.768667, Test_acc: 0.790000\n",
      "Train acc: 0.830625, Test_acc: 0.821250\n",
      "Train acc: 0.856167, Test_acc: 0.825000\n",
      "Train acc: 0.872333, Test_acc: 0.836250\n",
      "Train acc: 0.882792, Test_acc: 0.842500\n",
      "Train acc: 0.891042, Test_acc: 0.847500\n",
      "Train acc: 0.896042, Test_acc: 0.848750\n",
      "Train acc: 0.901000, Test_acc: 0.856250\n",
      "Train acc: 0.904500, Test_acc: 0.846250\n",
      "Train acc: 0.907292, Test_acc: 0.855000\n",
      "Train acc: 0.912083, Test_acc: 0.846250\n",
      "Train acc: 0.913917, Test_acc: 0.841250\n",
      "Train acc: 0.916875, Test_acc: 0.845000\n",
      "Train acc: 0.920250, Test_acc: 0.840000\n",
      "Train acc: 0.923292, Test_acc: 0.840000\n",
      "Train acc: 0.922958, Test_acc: 0.835000\n",
      "Train acc: 0.923500, Test_acc: 0.838750\n",
      "Train acc: 0.925208, Test_acc: 0.835000\n",
      "Train acc: 0.927167, Test_acc: 0.837500\n",
      "Train acc: 0.929292, Test_acc: 0.842500\n",
      "Train acc: 0.930750, Test_acc: 0.838750\n",
      "Train acc: 0.930292, Test_acc: 0.842500\n",
      "Train acc: 0.929875, Test_acc: 0.845000\n",
      "Train acc: 0.923583, Test_acc: 0.843750\n",
      "Train acc: 0.922583, Test_acc: 0.847500\n",
      "Train acc: 0.924625, Test_acc: 0.843750\n",
      "Train acc: 0.926000, Test_acc: 0.841250\n",
      "Train acc: 0.929458, Test_acc: 0.845000\n",
      "Train acc: 0.935583, Test_acc: 0.845000\n",
      "Train acc: 0.938167, Test_acc: 0.845000\n",
      "Train acc: 0.940125, Test_acc: 0.843750\n",
      "Train acc: 0.940667, Test_acc: 0.838750\n",
      "Train acc: 0.939833, Test_acc: 0.838750\n",
      "Train acc: 0.939042, Test_acc: 0.848750\n",
      "Train acc: 0.939500, Test_acc: 0.848750\n",
      "Train acc: 0.937375, Test_acc: 0.841250\n",
      "Train acc: 0.937292, Test_acc: 0.843750\n",
      "Train acc: 0.940625, Test_acc: 0.843750\n",
      "Train acc: 0.937000, Test_acc: 0.842500\n",
      "Train acc: 0.938583, Test_acc: 0.850000\n",
      "Train acc: 0.938208, Test_acc: 0.850000\n",
      "Train acc: 0.940500, Test_acc: 0.851250\n",
      "Train acc: 0.942458, Test_acc: 0.845000\n",
      "Train acc: 0.942833, Test_acc: 0.841250\n",
      "Train acc: 0.943667, Test_acc: 0.847500\n",
      "Train acc: 0.944208, Test_acc: 0.846250\n",
      "Train acc: 0.944167, Test_acc: 0.845000\n",
      "Train acc: 0.945250, Test_acc: 0.843750\n",
      "Train acc: 0.944500, Test_acc: 0.845000\n",
      "Time elapsed - 31.585865020751953 seconds.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(init)\n",
    "\n",
    "    h = h5py.File('data1.hdf5', 'r')\n",
    "    n1 = h.get('data_train') \n",
    "    n2 = h.get('data_test')\n",
    "\n",
    "    # Training cycle\n",
    "    total_batch_train = int(n1.shape[0] / batch_size)\n",
    "    total_batch_test = int(n2.shape[0] / batch_size)\n",
    "\n",
    "    for iter_num in range(num_epochs):\n",
    "        # variables for train and test accuracies\n",
    "        avg_acc_train = 0.\n",
    "        avg_acc_test = 0.\n",
    "        for i in range(total_batch_train):\n",
    "            train_x = n1[(i) * batch_size: (i + 1) * batch_size, ...]\n",
    "            train_y = labels_train[(i) * batch_size: (i + 1) * batch_size, :]\n",
    "\n",
    "            _, c_train, _logits_train = sess.run([train_op, loss_op, logits], feed_dict={X: train_x, Y: train_y})\n",
    "            _label_train = [np.argmax(i) for i in _logits_train]\n",
    "            _label_train_y = [np.argmax(i) for i in train_y]\n",
    "            _accuracy_train = np.mean(np.array(_label_train) == np.array(_label_train_y))\n",
    "            avg_acc_train += _accuracy_train\n",
    "\n",
    "\n",
    "        for j in range(total_batch_test):\n",
    "            test_x = n2[(j) * batch_size: (j + 1) * batch_size, ...]\n",
    "            test_y = labels_test[(j) * batch_size: (j + 1) * batch_size, :]\n",
    "\n",
    "            c_test, _logits_test = sess.run([loss_op, logits], feed_dict={X: test_x, Y: test_y})\n",
    "            _label_test = [np.argmax(i) for i in _logits_test]\n",
    "            _label_test_y = [np.argmax(i) for i in test_y]\n",
    "            _accuracy_test = np.mean(np.array(_label_test) == np.array(_label_test_y))\n",
    "            avg_acc_test += _accuracy_test\n",
    "\n",
    "        # print the train and test accuracies   \n",
    "        print(\"Train acc: %f, Test_acc: %f\" % (avg_acc_train/total_batch_train, avg_acc_test/total_batch_test))\n",
    "    duration = time.time() - start_time\n",
    "    print('Time elapsed - {} seconds.'.format(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_rluo9RAL9EI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "PA2-Part1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
